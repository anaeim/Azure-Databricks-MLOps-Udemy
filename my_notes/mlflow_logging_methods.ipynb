{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Summary of Logging Methods in MLflow for Databricks\n",
    "\n",
    "MLflow is an open-source platform used to manage the machine learning lifecycle, including experimentation, reproducibility, and deployment. In Databricks, MLflow integrates seamlessly with the platform to manage models and track experiments. Below is a summary of the key logging methods in MLflow used within Databricks.\n",
    "\n",
    "## 1. **Logging Parameters**\n",
    "Parameters are used to track hyperparameters, inputs, and configurations during model training.\n",
    "\n",
    "### Method:\n",
    "```python\n",
    "import mlflow\n",
    "\n",
    "# Log a parameter\n",
    "mlflow.log_param(\"param_name\", \"param_value\")\n",
    "```\n",
    "\n",
    "### Example:\n",
    "```python\n",
    "mlflow.log_param(\"learning_rate\", 0.01)\n",
    "mlflow.log_param(\"batch_size\", 32)\n",
    "```\n",
    "\n",
    "### 1.1 **Log multiple parameters**\n",
    "```python\n",
    "params = {\"batch_size\": 64, \"epochs\": 10, \"optimizer\": \"adam\"}\n",
    "mlflow.log_params(params)\n",
    "\n",
    "```\n",
    "\n",
    "## 2. **Logging Metrics**\n",
    "Metrics are numerical values that represent the modelâ€™s performance, such as accuracy, loss, and others.\n",
    "\n",
    "### Method:\n",
    "```python\n",
    "import mlflow\n",
    "\n",
    "# Log single metric\n",
    "mlflow.log_metric(\"accuracy\", 0.85)\n",
    "\n",
    "# Log metric at a specific step\n",
    "mlflow.log_metric(\"loss\", 0.12, step=1)\n",
    "\n",
    "# Log multiple metrics\n",
    "metrics = {\"precision\": 0.9, \"recall\": 0.82, \"f1\": 0.86}\n",
    "mlflow.log_metrics(metrics)\n",
    "```\n",
    "\n",
    "## 3. **Logging Artifacts**\n",
    "Artifacts can be files, directories, or other outputs generated during training (e.g., model weights, plots, or logs). These are useful for storing and reviewing outputs.\n",
    "\n",
    "### Method:\n",
    "```python\n",
    "import mlflow\n",
    "\n",
    "# Log an artifact (file or directory)\n",
    "mlflow.log_artifact(local_path, artifact_path=None)\n",
    "```\n",
    "\n",
    "### Example:\n",
    "```python\n",
    "mlflow.log_artifact(\"model_output.png\", \"outputs\")\n",
    "mlflow.log_artifact(\"logs/training_log.txt\")\n",
    "```\n",
    "\n",
    "## 4. **Logging Models**\n",
    "MLflow allows logging and tracking of models during training. It supports various model formats, including scikit-learn, TensorFlow, PyTorch, and custom models.\n",
    "\n",
    "### Method:\n",
    "```python\n",
    "import mlflow\n",
    "import mlflow.sklearn\n",
    "\n",
    "# Log a scikit-learn model\n",
    "mlflow.sklearn.log_model(model, \"model_name\")\n",
    "```\n",
    "\n",
    "### Example:\n",
    "```python\n",
    "mlflow.sklearn.log_model(model, \"random_forest_model\")\n",
    "```\n",
    "\n",
    "## 5. **Starting and Ending Runs**\n",
    "MLflow allows you to start a run to track an experiment and log metrics, parameters, and artifacts within that run. The run can be ended manually or automatically.\n",
    "\n",
    "### Methods:\n",
    "```python\n",
    "# Start a new run\n",
    "with mlflow.start_run():\n",
    "    # Log parameters, metrics, and artifacts\n",
    "    mlflow.log_param(\"epochs\", 10)\n",
    "    mlflow.log_metric(\"accuracy\", 0.95)\n",
    "    \n",
    "# End a run (automatic when 'with' block is used)\n",
    "```\n",
    "\n",
    "### Example:\n",
    "```python\n",
    "with mlflow.start_run():\n",
    "    mlflow.log_param(\"epochs\", 20)\n",
    "    mlflow.log_metric(\"accuracy\", 0.92)\n",
    "```\n",
    "\n",
    "## 6. **Tracking Experiments**\n",
    "In Databricks, MLflow organizes runs within experiments. You can set the active experiment and track multiple runs under different experiments.\n",
    "\n",
    "### Method:\n",
    "```python\n",
    "import mlflow\n",
    "\n",
    "# Set the active experiment\n",
    "mlflow.set_experiment(\"experiment_name\")\n",
    "\n",
    "# Log parameters and metrics within the experiment\n",
    "mlflow.start_run()\n",
    "mlflow.log_param(\"batch_size\", 64)\n",
    "mlflow.log_metric(\"accuracy\", 0.97)\n",
    "mlflow.end_run()\n",
    "```\n",
    "\n",
    "### Example:\n",
    "```python\n",
    "mlflow.set_experiment(\"/Users/your_username/experiment_1\")\n",
    "```\n",
    "\n",
    "## 7. **Using MLflow with Databricks Notebooks**\n",
    "In Databricks notebooks, you can use MLflow to automatically track the experiments associated with a particular notebook execution. Databricks integrates MLflow tracking UI directly into the workspace.\n",
    "\n",
    "### Key Points:\n",
    "- Use `mlflow.log_*` functions to log parameters, metrics, artifacts, and models.\n",
    "- The **MLflow UI** in Databricks allows you to visualize and compare different runs in an experiment.\n",
    "- Databricks users can leverage the `MLflow` tracking capabilities directly in the workspace without additional setup.\n",
    "\n",
    "\n",
    "## 8. **Model Signature Logging**\n",
    "```python\n",
    "from mlflow.models.signature import infer_signature\n",
    "\n",
    "# Infer model signature from data\n",
    "signature = infer_signature(X_train, model.predict(X_train))\n",
    "\n",
    "# Log model with signature\n",
    "mlflow.sklearn.log_model(model, \"model\", signature=signature)\n",
    "```\n",
    "\n",
    "\n",
    "## 9. **Tags and Notes**\n",
    "```python\n",
    "# Add tags to a run\n",
    "mlflow.set_tag(\"model_type\", \"classification\")\n",
    "mlflow.set_tags({\"priority\": \"high\", \"team\": \"data_science\"})\n",
    "\n",
    "# Add a note to the run\n",
    "mlflow.set_tag(\"mlflow.note.content\", \"This run uses the improved feature engineering pipeline\")\n",
    "```\n",
    "\n",
    "\n",
    "## Conclusion\n",
    "MLflow provides essential tools for logging parameters, metrics, artifacts, and models, and integrates seamlessly into Databricks for a streamlined machine learning workflow. The ability to track and compare different experiments allows data scientists and engineers to optimize and manage their models efficiently."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q. how to Set run descriptions?\n",
    "> Add detailed descriptions to document experiment purpose\n",
    "\n",
    "Using set_tag with the description key. <br>\n",
    "The most common way to add a run description is by using the `mlflow.note.content tag`:\n",
    "\n",
    "```python\n",
    "import mlflow\n",
    "\n",
    "with mlflow.start_run() as run:\n",
    "    # Set a detailed description for the run\n",
    "    mlflow.set_tag(\"mlflow.note.content\", \"\"\"\n",
    "    This experiment tests the impact of feature engineering on model performance.\n",
    "    We're comparing three different approaches:\n",
    "    1. Raw features only\n",
    "    2. Engineered features (polynomial features)\n",
    "    3. PCA-reduced features\n",
    "    \n",
    "    Expected outcome: Approach #2 should perform best on test data\n",
    "    while maintaining interpretability.\n",
    "    \"\"\")\n",
    "    \n",
    "    # Rest of your experiment code\n",
    "    mlflow.log_param(\"learning_rate\", 0.01)\n",
    "    # etc.\n",
    "```\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
